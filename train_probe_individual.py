"""
Trains a linear probing classifier over the hidden states generated by save_specific_letters_state.py
"""
import json
import h5py
import numpy as np
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import DataLoader, Dataset
import lightning as pl
from torch import nn
from torch.nn import functional as F
import matplotlib.pyplot as plt
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from lightning.pytorch.tuner import Tuner

# File paths
metadata_file = "letter_e_short_count_metadata.jsonl"
hidden_states_file = "letter_e_short_count_hidden_states.h5"

# Load metadata


def load_metadata(metadata_file):
    with open(metadata_file, "r") as f:
        metadata = [json.loads(line.strip()) for line in f]
    return metadata

# Load hidden states (shape: [N, num_layers, hidden_state_dim])


def load_hidden_states(hidden_states_file):
    with h5py.File(hidden_states_file, "r") as hf:
        count_subject_hidden_states = np.array(
            hf["count_subject_hidden_states"][:])
    return count_subject_hidden_states


metadata = load_metadata(metadata_file)
count_subject_hidden_states = load_hidden_states(hidden_states_file)

# Extract raw targets and map them to contiguous labels
raw_targets = np.array([row["count"] for row in metadata])
unique_labels = np.unique(raw_targets)
mapping = {label: idx for idx, label in enumerate(sorted(unique_labels))}
mapped_targets = np.array([mapping[label] for label in raw_targets])
num_classes = len(unique_labels)
print(f"Mapping of raw targets to classes: {mapping}")
print(f"Number of classes: {num_classes}")

# Split data into train, validation, and test:
# First, split out 100 examples for validation
X_temp, X_val, y_temp, y_val = train_test_split(
    count_subject_hidden_states, mapped_targets, test_size=200, random_state=42
)
# Then, split out 300 examples for test from the remaining data
X_train, X_test, y_train, y_test = train_test_split(
    X_temp, y_temp, test_size=500, random_state=42
)

# Define PyTorch Dataset for a given layer representation


class HiddenStatesDataset(Dataset):
    def __init__(self, hidden_states, targets):
        self.hidden_states = hidden_states
        self.targets = targets

    def __len__(self):
        return len(self.hidden_states)

    def __getitem__(self, idx):
        return (
            torch.tensor(self.hidden_states[idx], dtype=torch.float32),
            torch.tensor(self.targets[idx], dtype=torch.long),
        )

# Define the probing classifier (a simple linear layer)


class ProbingClassifier(pl.LightningModule):
    def __init__(self, input_dim, num_classes, lr=1e-3):
        super().__init__()
        self.fc = nn.Linear(input_dim, num_classes)
        self.lr = lr

    def forward(self, x):
        return self.fc(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)
        self.log("val_loss", loss, prog_bar=True)
        preds = torch.argmax(logits, dim=1)
        acc = (preds == y).float().mean()
        self.log("val_acc", acc, prog_bar=True)
        return loss

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        preds = torch.argmax(logits, dim=1)
        acc = (preds == y).float().mean()
        self.log("test_acc", acc, prog_bar=True)
        return {"test_acc": acc}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.lr)


# Determine dimensions (using each layer's vector independently)
num_layers = X_train.shape[1]
hidden_state_dim = X_train.shape[2]
input_dim = hidden_state_dim

# Lists to hold test metrics for the main probe
layer_accuracies = []

# Train one classifier per layer (using train, validation, and test splits)
for layer in range(num_layers):
    print(f"\nTraining classifier for layer {layer + 1}/{num_layers}")
    # Extract layer-specific representations: shape (N, hidden_state_dim)
    train_dataset = HiddenStatesDataset(X_train[:, layer], y_train)
    val_dataset = HiddenStatesDataset(X_val[:, layer], y_val)
    test_dataset = HiddenStatesDataset(X_test[:, layer], y_test)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    model_probe = ProbingClassifier(input_dim, num_classes)
    trainer = pl.Trainer(
        max_epochs=20,
        log_every_n_steps=1,
        devices=1,
        callbacks=[EarlyStopping(monitor="val_acc", mode="max")]
    )
    tuner = Tuner(trainer)
    lr_finder = tuner.lr_find(model_probe, train_dataloaders=train_loader)
    print("LR Finder results:", lr_finder.results)
    trainer.fit(model_probe, train_loader, val_loader)
    test_result = trainer.test(model_probe, test_loader, verbose=False)
    acc = test_result[0]["test_acc"]
    print(f"Layer {layer + 1} Accuracy: {acc:.4f}")
    layer_accuracies.append(acc)

# For baseline, train a probe using random labels
random_train_labels = np.random.randint(0, num_classes, size=len(y_train))
random_val_labels = np.random.randint(0, num_classes, size=len(y_val))
random_test_labels = np.random.randint(0, num_classes, size=len(y_test))

baseline_accuracies = []

for layer in range(num_layers):
    print(
        f"\nTraining baseline classifier for layer {layer + 1}/{num_layers} (random labels)")
    train_dataset = HiddenStatesDataset(X_train[:, layer], random_train_labels)
    val_dataset = HiddenStatesDataset(X_val[:, layer], random_val_labels)
    test_dataset = HiddenStatesDataset(X_test[:, layer], random_test_labels)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    model_baseline = ProbingClassifier(input_dim, num_classes)
    trainer = pl.Trainer(
        max_epochs=20,
        log_every_n_steps=1,
        devices=1,
        callbacks=[EarlyStopping(monitor="val_acc", mode="max")]
    )
    tuner = Tuner(trainer)
    lr_finder = tuner.lr_find(model_baseline, train_dataloaders=train_loader)
    print("LR Finder results:", lr_finder.results)
    trainer.fit(model_baseline, train_loader, val_loader)
    test_result = trainer.test(model_baseline, test_loader, verbose=False)
    acc = test_result[0]["test_acc"]
    print(f"Baseline Layer {layer + 1} Accuracy: {acc:.4f}")
    baseline_accuracies.append(acc)

# Print results for both main and baseline classifiers
print("\nMain Classifier Results:")
for i, acc in enumerate(layer_accuracies):
    print(f"Layer {i + 1}: Accuracy = {acc:.4f}")

print("\nBaseline Classifier Results:")
for i, acc in enumerate(baseline_accuracies):
    print(f"Layer {i + 1}: Accuracy = {acc:.4f}")

# Compute global min/max for accuracy from both probes for consistent heatmap scaling
combined_acc = np.concatenate(
    [np.array(layer_accuracies), np.array(baseline_accuracies)])
global_min = combined_acc.min()
global_max = combined_acc.max()

# Create two heatmaps side by side (one for main and one for baseline)
fig, axes = plt.subplots(1, 2, figsize=(12, 4), constrained_layout=True)

# Heatmap for main classifier Accuracy
main_acc_img = np.array(layer_accuracies)[np.newaxis, :]
im0 = axes[0].imshow(main_acc_img, cmap="coolwarm", aspect="auto",
                     vmin=global_min, vmax=global_max)
axes[0].set_xticks(np.arange(num_layers))
axes[0].set_xticklabels(
    [f"Layer {i+1}" for i in range(num_layers)], rotation=45, ha="right")
axes[0].set_yticks([])
axes[0].set_title("Main Classifier: Accuracy Across Layers")

# Heatmap for baseline classifier Accuracy
baseline_acc_img = np.array(baseline_accuracies)[np.newaxis, :]
im1 = axes[1].imshow(baseline_acc_img, cmap="coolwarm", aspect="auto",
                     vmin=global_min, vmax=global_max)
axes[1].set_xticks(np.arange(num_layers))
axes[1].set_xticklabels(
    [f"Layer {i+1}" for i in range(num_layers)], rotation=45, ha="right")
axes[1].set_yticks([])
axes[1].set_title("Baseline Classifier: Accuracy Across Layers")

# Add a common colorbar
cbar = fig.colorbar(im1, ax=axes, orientation="horizontal",
                    fraction=0.05, pad=0.1)
cbar.set_label("Accuracy")

plt.savefig("probe_heatmap_comparison.png")
plt.show()
